{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is dev branch\n",
    "import datetime\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "NUM_TIME_SLOTS = 144\n",
    "NUM_DAYS_IN_DATA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 hours is divided into 144 slots where each slot is 10 mins long\n",
    "def calculateTimeSlot(time,printValue=True):\n",
    "    global NUM_TIME_SLOTS\n",
    "    dateTime = datetime.datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "    timePart = dateTime.time()\n",
    "    timeInMinutes = (timePart.hour * 60) + timePart.minute + (timePart.second/60) + 1\n",
    "    timeSlot = timeInMinutes/10\n",
    "    roundedTimeSlot = math.ceil(timeSlot)\n",
    "    if roundedTimeSlot > NUM_TIME_SLOTS:\n",
    "        roundedTimeSlot -= 1\n",
    "    if printValue==True:\n",
    "        print(f\"time: {time} timeInMinutes: {timeInMinutes} timeSlot: {roundedTimeSlot}\")\n",
    "    return int(roundedTimeSlot)\n",
    "\n",
    "def extractDayOfWeek(time):\n",
    "    dateTime = datetime.datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "    return dateTime.weekday()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMultipleData(path,fileNamePrefix,headerNames,dataTypes):\n",
    "    global NUM_DAYS_IN_DATA\n",
    "    filesToExplore = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.startswith(fileNamePrefix):\n",
    "            filesToExplore.append(file)\n",
    "            # print(f\"{file} read\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(f\"{len(filesToExplore)} files read\")\n",
    "    if fileNamePrefix == 'order':\n",
    "        NUM_DAYS_IN_DATA = len(filesToExplore)\n",
    "    \n",
    "    readData = []\n",
    "    for files in filesToExplore:\n",
    "        fileRead = pd.read_csv(path + files, sep='\\t', names=headerNames,dtype=dataTypes)\n",
    "        readData.append(fileRead)\n",
    "\n",
    "    readData = pd.concat(readData, ignore_index=True)\n",
    "    return readData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now region Data\n",
    "regionData = pd.read_csv('./training_data/cluster_map/cluster_map', sep='\\t', names=['region_hash', 'region_id'],dtype={'region_hash': 'str', 'region_id': 'int'})\n",
    "# print(regionData.head())\n",
    "regionData.to_csv('regionData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 files read\n",
      "printing order data\n",
      "21 files read\n",
      "printing weather data\n"
     ]
    }
   ],
   "source": [
    "# read order data\n",
    "dataTypes = {'order_id':'str', 'driver_id':'str', 'passenger_id':'str', 'start_region_hash':'str', 'dest_region_hash':'str', 'price':'double', 'time':'str'}\n",
    "orderDataPath = './training_data/order_data/'\n",
    "orderData = readMultipleData(orderDataPath,'order', ['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'price', 'time'], dataTypes)\n",
    "print(\"printing order data\")\n",
    "# print(orderData.head())\n",
    "\n",
    "# read weather data\n",
    "dataTypes={'time':'str', 'weather':'int', 'temperature':'double', 'PM2.5':'double'}\n",
    "weatherDataPath = './training_data/weather_data/'\n",
    "weatherData = readMultipleData(weatherDataPath,'weather', ['time', 'weather', 'temperature', 'PM2.5'], dataTypes)\n",
    "print(\"printing weather data\")\n",
    "# print(weatherData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time  weather\n",
      "0     1        1\n",
      "1     1        1\n",
      "2     2        1\n",
      "3     2        1\n",
      "4     3        1\n"
     ]
    }
   ],
   "source": [
    "weatherData['time'] = weatherData['time'].apply(calculateTimeSlot,printValue=False)\n",
    "weatherData = weatherData.drop(['temperature','PM2.5'], axis=1)\n",
    "print(weatherData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   region_id                          order_id  \\\n",
      "0          1  1654babc363bc6d0f5d01fc0bafedc1a   \n",
      "1          1  5ac4ac8d0e6092ea1dc323d367613ffd   \n",
      "2          1  ce86a6ae5eee7a2ea954323b6c01510b   \n",
      "3          1  b5e816c08e44565c7ed67a6f6e366708   \n",
      "4          1  25ed10b13aaa36071deecab3aa374be3   \n",
      "\n",
      "                          driver_id                 time  time_slot  \\\n",
      "0  cc26812d679c9e55a6bf63eed315e989  2016-01-01 20:49:15        126   \n",
      "1  f6c760be3cd8521c612657da7788f9dc  2016-01-01 13:04:32         79   \n",
      "2  360478560b1fd4b3eb757074c91ee709  2016-01-01 21:00:21        127   \n",
      "3  0359fc335d238c6206703d1d7e3620c8  2016-01-01 19:25:32        117   \n",
      "4  c03944aff7444c27fd7b04cdd3e80af5  2016-01-01 20:35:36        124   \n",
      "\n",
      "   day_of_week  \n",
      "0            4  \n",
      "1            4  \n",
      "2            4  \n",
      "3            4  \n",
      "4            4  \n"
     ]
    }
   ],
   "source": [
    "orderData = pd.merge(regionData,orderData, how='left', right_on='start_region_hash', left_on='region_hash')\n",
    "orderData['time_slot'] = orderData['time'].apply(calculateTimeSlot,printValue=False)\n",
    "orderData['day_of_week'] = orderData['time'].apply(extractDayOfWeek)\n",
    "orderData = orderData.drop(['passenger_id', 'dest_region_hash','start_region_hash','price','region_hash'], axis=1)\n",
    "print(orderData.head())\n",
    "regionData=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.00 GiB for an array with shape (267815373, 1) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17832\\3809378434.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0morderData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweatherData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morderData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"inner\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time_slot'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morderData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0morderData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'orderData.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     )\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m         result = self._reindex_and_concat(\n\u001b[0m\u001b[0;32m    776\u001b[0m             \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m             rmgr = right._mgr.reindex_indexer(\n\u001b[0m\u001b[0;32m    751\u001b[0m                 \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[0mright_indexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    751\u001b[0m             \u001b[0mparent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_none\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_refs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             new_blocks = [\n\u001b[0m\u001b[0;32m    754\u001b[0m                 blk.take_nd(\n\u001b[0;32m    755\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             new_blocks = [\n\u001b[1;32m--> 754\u001b[1;33m                 blk.take_nd(\n\u001b[0m\u001b[0;32m    755\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         new_values = algos.take_nd(\n\u001b[0m\u001b[0;32m    881\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pc\\anaconda3\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.00 GiB for an array with shape (267815373, 1) and data type int64"
     ]
    }
   ],
   "source": [
    "orderData = pd.merge(weatherData,orderData, how=\"inner\", left_on='time', right_on='time_slot')\n",
    "print(orderData)\n",
    "orderData.to_csv('orderData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(orderData['driver_id'][4])==float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read POI Data\n",
    "poiDataStr = {\n",
    "    'region_hash':[],\n",
    "    'poi_class':[]\n",
    "}\n",
    "with open('./training_data/poi_data/poi_data','r') as fileToRead:\n",
    "    for line in fileToRead:\n",
    "        line = line.strip()\n",
    "        columns = line.split('\\t')\n",
    "        poiDataStr['region_hash'].append(columns[0])\n",
    "        remData = columns[1:]\n",
    "        poiDataStr['poi_class'].append(remData)\n",
    "        \n",
    "poiData = pd.DataFrame(poiDataStr,columns=['region_hash','poi_class'])\n",
    "# print(\"printing poi data\")\n",
    "# print(poiData.head())\n",
    "# print(\"Printing poi data line 1\")\n",
    "# print(f\"region_hash: {poiData['region_hash'][0]} poi_class: {poiData['poi_class'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orderData = pd.merge(orderData,poiData, how=\"outer\",right=\"region_hash\",left=\"start_region_hash\")\n",
    "# print(orderData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to calculate gap(i,j) = req(i,j) - supply(i,j)\n",
    "# req(i,j) is for region i and timeslot j \n",
    "# ith region will be from from start_region_hash and jth timeslot will be calculated from time\n",
    "def getRegionID(regionHash):\n",
    "    regionID = -1\n",
    "    for i in range(len(regionData)):\n",
    "        if regionHash == regionData['region_hash'][i]:\n",
    "            regionID = regionData['region_id'][i]\n",
    "    return regionID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = None # here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop order_id, driver_id, passenger_id, dest_region_hash\n",
    "# mergedData = orderData.drop(['order_id', 'passenger_id', 'dest_region_hash'], axis=1)\n",
    "# print(\"dropped order_id, passenger_id, dest_region_hash\")\n",
    "#  merge order data with region data on start_region_hash with region_hash\n",
    "# mergedData = pd.merge(regionData,mergedData, how='left', right_on='start_region_hash', left_on='region_hash')\n",
    "# print(\"merged order data and region data based on region\")\n",
    "\n",
    "# mergedData = mergedData.drop(['region_hash','start_region_hash'], axis=1)\n",
    "# print(\"dropped region_hash, start_region_hash\")\n",
    "# # reduce time to time slot and update time column\n",
    "# mergedData['time'] = mergedData['time'].apply(calculateTimeSlot,printValue=False)\n",
    "# print(\"reduced time to time slot 1 to 144\")\n",
    "# rename time to time_slot\n",
    "# mergedData.rename(columns={'time':'time_slot'}, inplace=True)\n",
    "# # append column for day of week into mergedData\n",
    "# mergedData['day_of_week'] = orderData['time'].apply(extractDayOfWeek)\n",
    "# print(\"appended day_of_week column to data\")\n",
    "# now we have mergedData with region_id, price, time, day_of_week\n",
    "# print(\"printing merged data\")\n",
    "# print(mergedData)\n",
    "\n",
    "# writing to mergedData.csv for quick access\n",
    "orderData.to_csv('mergedData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mergedData.csv\n",
    "mergedDataCSV = pd.read_csv('mergedData.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mergedDataCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mergedDataCSV)\n",
    "mergedDataCSV['requests'] = 1\n",
    "groupedMergedDataCSV = mergedDataCSV.groupby(['region_id','time_slot','day_of_week','weather','temperature','PM2.5'])['requests'].agg('sum').reset_index()\n",
    "mergedData = pd.merge(mergedDataCSV,groupedMergedDataCSV, how='left')\n",
    "print(\"printing grouped merged data\")\n",
    "# print(mergedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now to get req(i,j) we can do that by counting the number of orders for region i and timeslot j\n",
    "# def getRequest(i,j): # i is region id and j is timeslot\n",
    "#     global orderData\n",
    "#     numberOfIterations = len(orderData)\n",
    "#     print(f\"Number of lines of data: {numberOfIterations}\")\n",
    "#     progressBarInit = tqdm(total=numberOfIterations, desc=\"Calculating requests\", unit=\" lines\")\n",
    "#     requests = 0\n",
    "#     for row in range(len(orderData)):\n",
    "#         currentRegionID = getRegionID(orderData['start_region_hash'][row])\n",
    "#         currentTimeSlot = calculateTimeSlot(orderData['time'][row],False)\n",
    "#         if currentRegionID == i and currentTimeSlot == j:\n",
    "#             requests += 1\n",
    "#         progressBarInit.update(1)\n",
    "#     progressBarInit.close()\n",
    "#     return requests\n",
    "\n",
    "# print(\"Printing request for 1st region and 1st timeslot\")\n",
    "# print(getRequest(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def dateToIndex(date):\n",
    "# #     index = 0\n",
    "    \n",
    "# #     return index\n",
    "\n",
    "# def getAllRequestAndSupply(): # need to filter by date \n",
    "#     global orderData\n",
    "#     global regionData\n",
    "#     global NUM_TIME_SLOTS\n",
    "#     global NUM_DAYS_IN_DATA\n",
    "#     numberOfRegions = len(regionData)\n",
    "#     numberOfIterations = len(orderData)\n",
    "#     print(f\"Number of lines of data: {numberOfIterations}\")\n",
    "#     progressBarInit = tqdm(total=numberOfIterations, desc=\"Calculating requests\", unit=\" lines\")\n",
    "#     # 3D requests array requests[i][j][k] is number of requests --> date i ,region j, timeslot k\n",
    "#     # requests = [[[0 for k in range(NUM_TIME_SLOTS)] for j in range(numberOfRegions)] for i in range(NUM_DAYS_IN_DATA)]\n",
    "#     # supply = [[[0 for k in range(NUM_TIME_SLOTS)] for j in range(numberOfRegions)] for i in range(NUM_DAYS_IN_DATA)]\n",
    "#     requests = [[0 for j in range(NUM_TIME_SLOTS)] for i in range(numberOfRegions)]\n",
    "#     supply = [[0 for j in range(NUM_TIME_SLOTS)] for i in range(numberOfRegions)]\n",
    "#     for row in range(len(orderData)):\n",
    "#         currentRegionID = getRegionID(orderData['start_region_hash'][row])\n",
    "#         currentTimeSlot = calculateTimeSlot(orderData['time'][row],False)\n",
    "#         # date = orderData['time'][row].split(' ')[0]\n",
    "        \n",
    "#         if currentRegionID < 0:\n",
    "#             print(f\"Region not found for {orderData['start_region_hash'][row]}\")\n",
    "#             continue\n",
    "#             # return (None,None)\n",
    "#         if currentTimeSlot < 0:\n",
    "#             print(f\"Time slot not found for {orderData['time'][row]}\")\n",
    "#             continue\n",
    "#             # return (None,None)\n",
    "#         if currentRegionID > numberOfRegions:\n",
    "#             print(f\"Region id {currentRegionID} is greater than number of regions {numberOfRegions}\")\n",
    "#             continue\n",
    "#             # return (None,None)\n",
    "#         if currentTimeSlot > NUM_TIME_SLOTS:\n",
    "#             print(f\"Time slot {currentTimeSlot} is greater than number of time slots {NUM_TIME_SLOTS}\")\n",
    "#             print(f\"Time: {orderData['time'][row]}\")\n",
    "#             print(f\"Row: {row}\")\n",
    "#             continue\n",
    "#             # return (None,None)\n",
    "#         if currentTimeSlot == 0:\n",
    "#             print(f\"Time slot is 0 for {orderData['time'][row]}\")\n",
    "#             print(f\"Row: {row}\")\n",
    "#             continue\n",
    "#             # return (None,None)\n",
    "#         # requests[currentDate][currentRegionID-1][currentTimeSlot-1] += 1\n",
    "#         requests[currentRegionID-1][currentTimeSlot-1] += 1\n",
    "#         if type(orderData['driver_id'][row]) == str:\n",
    "#             supply[currentRegionID-1][currentTimeSlot-1] += 1\n",
    "#             # supply[currentDate][currentRegionID-1][currentTimeSlot-1] += 1\n",
    "#         progressBarInit.update(1)\n",
    "#     progressBarInit.close()\n",
    "#     return (requests,supply)\n",
    "\n",
    "# print(\"Printing request and supply regions d(i) and timeslots t(j)\")\n",
    "# (request,supply) = getAllRequestAndSupply()\n",
    "# print(request)\n",
    "# print(supply) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npRequest = np.array(request)\n",
    "# npSupply = np.array(supply)\n",
    "# np.savetxt('request.csv', npRequest, delimiter=',')\n",
    "# np.savetxt('supply.csv', npSupply, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now to get req(i,j) we can do that by counting the number of orders for region i and timeslot j\n",
    "# # concurrentI =0\n",
    "# # concurrentJ =0\n",
    "# def getRequestMulti(data, i, j,lowerIndex,upperIndex):\n",
    "#     # (orderData, i, j,lowerIndex,upperIndex) = arguments\n",
    "#     numberOfIterations = upperIndex - lowerIndex\n",
    "#     currentPID = mp.current_process()._identity[0]-1\n",
    "#     # logging.info(f\"process {currentPID}\")\n",
    "#     print(f\"Number of lines of data: {numberOfIterations} for process {currentPID}\")\n",
    "#     # progressBarInit = tqdm(total=numberOfIterations, desc=f\"Calculating requests {currentPID}\", unit=\" lines\")\n",
    "#     lowerIndex = lowerIndex[currentPID]\n",
    "#     upperIndex = upperIndex[currentPID]\n",
    "#     requests = 0\n",
    "#     for row in range(lowerIndex,upperIndex):\n",
    "#         currentRegionID = getRegionID(data['start_region_hash'][row])\n",
    "#         currentTimeSlot = calculateTimeSlot(data['time'][row],False)\n",
    "#         if currentRegionID == i and currentTimeSlot == j:\n",
    "#             requests += 1\n",
    "#         # progressBarInit.update(1)\n",
    "#     # progressBarInit.close()\n",
    "#     return requests\n",
    "\n",
    "# def getRequestHelper(i,j): # i is region id and j is timeslot\n",
    "#     global orderData\n",
    "#     # global concurrentI\n",
    "#     # global concurrentJ\n",
    "#     # concurrentJ = j\n",
    "#     # concurrentI = i\n",
    "#     numberOfIterations = len(orderData)\n",
    "#     # logging.basicConfig(level=logging.INFO,filename='worker.log', filemode='w')\n",
    "#     # console_handler = logging.StreamHandler()\n",
    "#     # logging.getLogger().addHandler(console_handler)\n",
    "#     # print(f\"Number of lines of data: {numberOfIterations}\")\n",
    "#     # progressBarInit = tqdm(total=numberOfIterations, desc=\"Calculating requests\", unit=\" lines\")\n",
    "    \n",
    "#     numberOfProcessesToRun = mp.cpu_count()\n",
    "#     print(f\"CPUs: {numberOfProcessesToRun}\")\n",
    "#     multiProcessingPool = mp.Pool(numberOfProcessesToRun)\n",
    "#     upperIndex = []\n",
    "#     lowerIndex = []\n",
    "#     for i in range(numberOfProcessesToRun):\n",
    "#         lowerval = i*numberOfIterations//numberOfProcessesToRun\n",
    "#         upperVal = (i+1)*numberOfIterations//numberOfProcessesToRun\n",
    "#         lowerIndex.append(lowerval)\n",
    "#         upperIndex.append(upperVal)\n",
    "#     print(\"Here\")\n",
    "#     # argumentsToPass = (orderData, i, j,lowerIndex, upperIndex)\n",
    "#     requests = multiProcessingPool.starmap(getRequestMulti, [(orderData, i, j,lowerIndex, upperIndex)])\n",
    "#     requests = sum(requests)\n",
    "#     multiProcessingPool.close()\n",
    "#     multiProcessingPool.join()\n",
    "#     return requests\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Printing request for 1st region and 1st timeslot\")\n",
    "# print(getRequestHelper(1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
